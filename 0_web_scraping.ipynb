{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webdriver - open the page and click on \"Accept all\"\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import html.parser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_rate_time(result_set):\n",
    "    rev_dict = {'Review Rate': [],\n",
    "                'Review Time': []\n",
    "                }\n",
    "    for result in result_set:\n",
    "        # find the review and time except when it is None type \n",
    "        try: \n",
    "            review_rate = result.find('span', class_='kvMYJc')['aria-label']\n",
    "            rev_dict['Review Rate'].append(review_rate)\n",
    "\n",
    "            review_time = result.find('span', class_='rsqaWe').text\n",
    "            rev_dict['Review Time'].append(review_time)\n",
    "        except TypeError or AttributeError:\n",
    "            pass\n",
    "\n",
    "    return(pd.DataFrame(rev_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_text(result_set):\n",
    "    rev_dict = {'Review Text': []\n",
    "                }\n",
    "    for result in result_set: \n",
    "        # get the review and only get the odd line (even lines are all None)\n",
    "        review_text = result.find('span', class_='wiI7pd')\n",
    "        if review_text is not None:\n",
    "            review_text_content = review_text.text\n",
    "            rev_dict['Review Text'].append(review_text_content)\n",
    "        else: \n",
    "            rev_dict['Review Text'].append(None)\n",
    "    return(pd.DataFrame(rev_dict).iloc[lambda x: x.index % 2==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wenxia\\AppData\\Local\\Temp\\ipykernel_16832\\588240425.py:18: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2118\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "url_list = {\n",
    "    # 'inglewood_plainpalais': 'https://www.google.ch/maps/place/Inglewood+Plainpalais/@46.1941363,6.1356256,17z/data=!3m1!4b1!4m6!3m5!1s0x478c7ad5ea8b6c3d:0xcab0358f5a95c111!8m2!3d46.1941364!4d6.140239!16s%2Fg%2F12hkt3wm3?hl=en'#,\n",
    "    #'inglewood_eaux_vives': 'https://www.google.ch/maps/place/Inglewood+Eaux-Vives/@46.2019106,6.1543685,17z/data=!3m1!4b1!4m6!3m5!1s0x478c6536f77925e7:0xed4637560bf9d338!8m2!3d46.2019106!4d6.1569434!16s%2Fg%2F12hksttms?hl=en',\n",
    "    #'inglewood_nyon': 'https://www.google.ch/maps/place/Inglewood+Nyon/@46.3841267,6.2336704,17z/data=!3m1!4b1!4m6!3m5!1s0x478c434c98ccd95f:0x502556c0d7231f8e!8m2!3d46.3841267!4d6.2362453!16s%2Fg%2F11tdqd8g86?hl=en',\n",
    "    # 'inglewood_saint_laurent': 'https://www.google.ch/maps/place/Inglewood+Saint-Laurent/@46.5224191,6.613002,15z/data=!3m2!4b1!5s0x478c2e31fb50c131:0x1f39e5f530132aba!4m6!3m5!1s0x478c2e31f16f0a8f:0xb9d7991feb029033!8m2!3d46.5224206!4d6.6314558!16s%2Fg%2F11bvt4srnl?hl=en',\n",
    "    # 'holy_cow_plainpalais': 'https://www.google.ch/maps/place/Holy+Cow!+Gourmet+Burger+Co.+GEN%C3%88VE+PLAINPALAIS/@46.1967473,6.1244618,15z/data=!3m1!4b1!4m6!3m5!1s0x478c7ad4dcb1422d:0x99319b6addd1dd64!8m2!3d46.1967488!4d6.1429156!16s%2Fg%2F11x9gt295?hl=en',\n",
    "    # 'holy_cow_cornavin': 'https://www.google.ch/maps/place/Holy+Cow/@46.2103302,6.1416593,17z/data=!3m1!4b1!4m6!3m5!1s0x478c6527119302c7:0xb77114f95fafe45b!8m2!3d46.2103302!4d6.1442342!16s%2Fg%2F11b6_frwc3?hl=en',\n",
    "    # 'funky_monkey_room': 'https://www.google.ch/maps/place/Funky+Monkey+Room/@46.2033229,6.1332716,17z/data=!3m1!4b1!4m6!3m5!1s0x478c653427014cdd:0xfee2adcb821b70e4!8m2!3d46.2033229!4d6.1358465!16s%2Fg%2F1wk7prys?hl=en',\n",
    "    # 'hamburger_foundation_plainpalais': 'https://www.google.ch/maps/place/The+Hamburger+Foundation+-+Plainpalais/@46.1964521,6.1413115,17z/data=!3m1!4b1!4m6!3m5!1s0x478c7ad4c068e6c1:0xa55046e000f9e64d!8m2!3d46.1964521!4d6.1438864!16s%2Fg%2F11c2kx5jb4?hl=en',\n",
    "    # 'hamburger_foundation_paquis': 'https://www.google.ch/maps/place/The+Hamburger+Foundation+-+P%C3%A2quis/@46.2114407,6.1482427,17z/data=!3m1!4b1!4m6!3m5!1s0x478c65247eeb1a1f:0x4a93b6b7e35200a2!8m2!3d46.2114407!4d6.1508176!16s%2Fg%2F1q63ckph5?hl=en',\n",
    "    # 'hamburger_foundation_eaux_vives': 'https://www.google.ch/maps/place/The+Hamburger+Foundation+-+Eaux-Vives/@46.2039979,6.1553321,17z/data=!3m1!4b1!4m6!3m5!1s0x478c65d7af26ae0f:0x2cf166cdb2fdd639!8m2!3d46.2039979!4d6.157907!16s%2Fg%2F11h_xd81hh?hl=en',\n",
    "    'black_tap': 'https://www.google.ch/maps/place/Black+Tap/@46.2014093,6.1502367,17z/data=!3m1!4b1!4m6!3m5!1s0x478c6532199536c3:0x1fa6ca815c2ba92f!8m2!3d46.2014093!4d6.1528116!16s%2Fg%2F11fzyxrlws?hl=en'\n",
    "   }\n",
    "\n",
    "for url in url_list:\n",
    "\n",
    "    path = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "    driver = webdriver.Chrome(path)\n",
    "    \n",
    "    # Get the url link and access it \n",
    "    driver.get(url_list[url])\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Click on agree all to open the map\n",
    "    element = driver.find_element(By.XPATH, '//*[@id=\"yDmH0d\"]/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/div[1]/form[2]/div/div/button/span')\n",
    "    element.click()\n",
    "\n",
    "    # Get the total number of reviews \n",
    "    total_num_of_reviews = driver.find_element(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div/div[1]/div[2]/div/div[1]/div[2]/span[2]/span/span').text.split(' ')[0]\n",
    "    total_num_of_reviews = total_num_of_reviews.replace(',','').replace('(','').replace(')','')\n",
    "    total_num_of_reviews = int(total_num_of_reviews)\n",
    "    print(total_num_of_reviews)\n",
    "\n",
    "    # Google can load maximum 1139 reviews and then stops loading. \n",
    "    # We can divide the download into two tranches: 1) highest to lowest (1139 reviews); 2) lowest to hightest (the rest, maybe with some overlap)\n",
    "    # rest_number_of_reviews = total_num_of_reviews - 1129 \n",
    "    \n",
    "    # Click on 'More reviews'\n",
    "    more_reviews = driver.find_element(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]/div[2]/div[2]')\n",
    "    more_reviews.click()\n",
    "\n",
    "    # Click on \"sort\" then \"highest rating\"\n",
    "    sort = driver.find_element(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[8]/div[2]/button/span/span').click()\n",
    "    highest = driver.find_element(By.XPATH, '//*[@id=\"action-menu\"]/div[3]').click()\n",
    "\n",
    "    # Find the scrollable element\n",
    "    scrollable_div = driver.find_element(By.XPATH, \"//*[@id=\\\"QA0Szd\\\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]\") \n",
    "\n",
    "    # Scroll to load all reviews \n",
    "    for i in range(0, 113):\n",
    "        driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "        if i%10 == 0: print(i)\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Click on 'more' for each review\n",
    "    items = driver.find_elements(By.CSS_SELECTOR, '.w8nwRe.kyuRq')\n",
    "\n",
    "    for item in items:\n",
    "        item.click()\n",
    "        time.sleep(3)\n",
    "    \n",
    "    # Parsing HTML\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews = response.find_all('div', class_='GHT2ce')\n",
    "\n",
    "    # Get the reviews (from highest to lowest)\n",
    "    df_rate_time = get_review_rate_time(reviews)\n",
    "    df_text = get_review_text(reviews)\n",
    "    df_text.reset_index(drop=True, inplace=True)\n",
    "    frames = [df_rate_time, df_text]\n",
    "    df = pd.concat(frames, axis=1)\n",
    "\n",
    "    # Save to csv \n",
    "    df.to_csv(f'{url}_highest.csv', encoding='utf-8-sig')\n",
    "\n",
    "    # Click on \"sort\" then \"lowest rating\"\n",
    "    sort = driver.find_element(By.XPATH, '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[8]/div[2]/button/span/span').click()\n",
    "    highest = driver.find_element(By.XPATH, '//*[@id=\"action-menu\"]/div[4]').click()\n",
    "\n",
    "    # Find the scrollable element\n",
    "    scrollable_div = driver.find_element(By.XPATH, \"//*[@id=\\\"QA0Szd\\\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]\") \n",
    "\n",
    "    # Calculate how many times to scroll down (each scroll = 10 reviews)\n",
    "    # Scroll_num = round(rest_number_of_reviews/10) + 10 \n",
    "\n",
    "    # Scroll to load all reviews \n",
    "    for i in range(0, 113):\n",
    "        driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "        if i%10 == 0: print(i)\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Parsing HTML\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews = response.find_all('div', class_='GHT2ce')\n",
    "\n",
    "    df_rate_time = get_review_rate_time(reviews)\n",
    "    df_text = get_review_text(reviews)\n",
    "    df_text.reset_index(drop=True, inplace=True)\n",
    "    frames = [df_rate_time, df_text]\n",
    "    df = pd.concat(frames, axis=1)\n",
    "    df.to_csv(f'{url}_lowest.csv', encoding='utf-8-sig')\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt \n",
    "# nltk.download('wordnet')\n",
    "\n",
    "import time \n",
    "\n",
    "import gensim \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data, add location, and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current working directory\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# get the list of csv files \n",
    "csv_files = glob.glob(os.path.join(current_path, 'data', '*.csv'))\n",
    "\n",
    "maindf = pd.DataFrame()\n",
    "# loop over the list of csv files \n",
    "for file in csv_files:\n",
    "\n",
    "    # read the csv file\n",
    "    df = pd.read_csv(file, dtype={'Review Text': str})\n",
    "    \n",
    "    # get the file name and extract the location info \n",
    "    file_name = os.path.basename(file)\n",
    "    removedWord = ['inglewood', 'highest.csv', 'lowest.csv']\n",
    "    location = ' '.join(i for i in file_name.split('_') if i not in removedWord).title()\n",
    "    \n",
    "    # add a location column to the data frame \n",
    "    df['Location'] = location\n",
    "    # concat all the df \n",
    "    maindf = pd.concat([maindf, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the first column (old index from csv)\n",
    "maindf.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# drop duplicates \n",
    "maindf.drop_duplicates(subset = 'Review Text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "maindf.head()\n",
    "maindf.to_csv('merged_raw_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
